<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · StatReg.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>StatReg.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Home</a><ul class="internal"><li class="toplevel"><a class="toctext" href="#Description-of-statistical-regularisation’s-method-1">Description of statistical regularisation’s method</a></li><li class="toplevel"><a class="toctext" href="#Smoothness-as-a-prior-information-1">Smoothness as a prior information</a></li><li class="toplevel"><a class="toctext" href="#Gaussian-random-process-1">Gaussian random process</a></li><li class="toplevel"><a class="toctext" href="#Non-Gaussian-random-process-1">Non-Gaussian random process</a></li></ul></li><li><a class="toctext" href="getting_started/">Getting started</a></li><li><a class="toctext" href="users_guide/">User&#39;s Guide</a></li><li><a class="toctext" href="examples/">Examples</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Home</a></li></ul><a class="edit-page" href="https://github.com/mipt-npm/StatReg.jl/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Home</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Statreg.jl-1" href="#Statreg.jl-1">Statreg.jl</a></h1><p>This is documentation for <code>Statreg.jl</code> – a Julia package that allows to apply Turchin&#39;s method of statistical regularisation to solve the Fredholm equation of the first kind.</p><p>Let&#39;s consider equation</p><div>\[f(y) = \int\limits_{a}^{b} K(x, y) \varphi(x) dx\]</div><p>The problem is, given kernel function <span>$K(x, y)$</span> and observed function <span>$f(y)$</span>, to find the function <span>$\varphi(x)$</span>. <span>$f(y)$</span> contains a random noise factor both from initial statistical uncertainty of <span>$\varphi(x)$</span> and additional noise from measurement procedure. The equation is ill-posed: a small measurement error of <span>$f(y)$</span> leads to big instability of <span>$\varphi(x)$</span>. Solving such ill-posed problems requires operation called regularisation. It means we need to introduce additional information to make the problem well-posed one.</p><p>The idea of statistical regularisation is to look on the problem from the point of view of Bayesian statistics approach: unknown statistical value <span>$\varphi(x)$</span> could be reconstructed using measured value <span>$f(y)$</span>, the kernel <span>$K(x, y)$</span> and some prior information about <span>$\varphi(x)$</span> behaviour: smoothness, constraints on boundary conditions, non-negativity, etc. Also it is important to note that statistical regularisation allows to estimate errors of the obtained solution. More information about the theory of statistical regularisation you can find <a href="https://www.epj-conferences.org/articles/epjconf/abs/2018/12/epjconf_ayss2018_07005/epjconf_ayss2018_07005.html">here</a>, but the main concepts will be explained further in this documentation.</p><h1><a class="nav-anchor" id="Description-of-statistical-regularisation’s-method-1" href="#Description-of-statistical-regularisation’s-method-1">Description of statistical regularisation’s method</a></h1><p>Firstly, it is necessary to make parameterised discrete representation from the continuous functional space. We should introduce basis <span>$\{ \psi_k \}_{k=1}^N$</span>, in which the required function will be calculated. Then the Fredholm equation will be сonverted to the matrix equation:</p><div>\[f_m = K_{mn} \varphi_n,\]</div><p>where <span>$f_m = f(y_m)$</span>,  <span>$\varphi_n$</span> :  <span>$\varphi(x) = \sum\limits_{k=1}^{N} \varphi_k \psi_k(x)$</span>,  <span>$K_{mn} = \int\limits_{a}^{b} K(x, y_m) \psi_n(x) dx$</span>.</p><p>Let&#39;s introduce function <span>$\overrightarrow{S}$</span> that will evaluate <span>$\overrightarrow{\varphi}$</span> based on the function <span>$\overrightarrow{f}$</span> and loss function <span>$L(\overrightarrow{\widehat{\varphi}}, \overrightarrow{S}) = \sum\limits_{n=1}^{N} \mu_n (\widehat{\varphi}_n - S_n)^2$</span>, where <span>$\overrightarrow{\widehat{\varphi}}=\overrightarrow{\widehat{S}}(\overrightarrow{f})$</span> – the best solution.</p><p>For this loss function the best strategy is</p><div>\[\overrightarrow{\widehat{S}}[f]=E[\overrightarrow{\varphi}|\overrightarrow{f}]=\int \overrightarrow{\varphi} P(\overrightarrow{\varphi}|\overrightarrow{f}) d\overrightarrow{\varphi}\]</div><div>\[P(\overrightarrow{\varphi}|\overrightarrow{f}) = \frac{P(\overrightarrow{\varphi})P(\overrightarrow{f}|\overrightarrow{\varphi})}{\int d\overrightarrow{\varphi}P(\overrightarrow{\varphi})P(\overrightarrow{f}|\overrightarrow{\varphi})}\]</div><p>Errors of the solution:</p><div>\[&lt; \sigma_n^2 &gt; = \int (\varphi_n - \widehat{S}_n)^2 P(\overrightarrow{\varphi}|\overrightarrow{f})d\overrightarrow{\varphi}\]</div><p>So, <span>$P(\overrightarrow{\varphi})$</span> and <span>$P(\overrightarrow{f}|\overrightarrow{\varphi})$</span> are required to find the solution. <span>$P(\overrightarrow{\varphi})$</span> can be chosen using prior information about <span>$\overrightarrow{\varphi})$</span>. <span>$P(\overrightarrow{f}|\overrightarrow{\varphi})$</span> depends on <span>$\overrightarrow{f}$</span> distribution.</p><h1><a class="nav-anchor" id="Smoothness-as-a-prior-information-1" href="#Smoothness-as-a-prior-information-1">Smoothness as a prior information</a></h1><p>We expect <span>$\varphi(x)$</span> to be relatively smooth and can choose this information as a prior. The matrix of the mean value of derivatives of order <span>$p$</span> can be used as a prior information about the solution.</p><div>\[\Omega_{mn} = \int\limits_{a}^{b} \left( \frac{d^p \psi_m(x)}{dx^p} \right) \left( \frac{d^p \psi_n(x)}{dx^p} \right) dx\]</div><p>We require a certain value of the smoothness functional to be achieved:</p><div>\[\int (\overrightarrow{\varphi}, \Omega \overrightarrow{\varphi}) P(\overrightarrow{\varphi})d\overrightarrow{\varphi}=\omega\]</div><p>Thus, the <span>$\overrightarrow{\varphi}$</span> probability distribution depends on the parameter:</p><div>\[P_{\alpha}(\overrightarrow{\varphi})=\frac{\alpha^{Rg(\Omega)/2} \sqrt{det(\Omega)}}{(2\pi)^{N/2}}exp\left( -\frac{1}{2} (\overrightarrow{\varphi}, \Omega \overrightarrow{\varphi}) \right)\]</div><p>where <span>$\alpha=\frac{1}{\omega}$</span>.</p><p>The value of the parameter <span>$\alpha$</span> is unknown, and can be obtained in the following ways:</p><ul><li>directly from some external data or manually selected</li><li>as a maximum of a posterior information <span>$P(\alpha|\overrightarrow{f})$</span></li><li>as the mean of all possible <span>$\alpha$</span>, defining the prior probability density as <span>$P(\overrightarrow{\varphi})=\int d\alpha P(\alpha) P(\overrightarrow{\varphi}|\alpha)$</span> (all alphas are equally probable).</li></ul><h1><a class="nav-anchor" id="Gaussian-random-process-1" href="#Gaussian-random-process-1">Gaussian random process</a></h1><p>The most common case is when the variation of the experimental results is subject to the normal distribution. At that rate the regularisation has an analytical solution. Let the measurement vector f have errors described by a multidimensional Gaussian distribution with a covariance matrix <span>$\Sigma$</span>:</p><div>\[P(\overrightarrow{f}|\overrightarrow{\varphi})=\frac{1}{(2\pi)^{N/2}|\Sigma|^{1/2}}exp\left( -\frac{1}{2} (\overrightarrow{f} - K\overrightarrow{\varphi})^T \Sigma^{-1} (\overrightarrow{f} - K\overrightarrow{\varphi}) \right)\]</div><p>Using the most probable <span>$\alpha$</span>, one can get the best solution:</p><div>\[\overrightarrow{\widehat{S}} = (K^T \Sigma^{-1} K + \alpha^* \Omega)^{-1} K^T \Sigma^{-1 T} \overrightarrow{f}\]</div><div>\[cov(\varphi_m, \varphi_n) = ||(K^T \Sigma^{-1} K + \alpha^* \Omega)^{-1}||_{mn}\]</div><p>This package allows to apply statistical regularisation in different bases using such a prior information as smoothness and zero boundary conditions or another information provided by user in a matrix form. <span>$\Omega$</span> can be set manually or calculated for every derivative  of degree <span>$p$</span>. <span>$\alpha$</span> can be calculated as a maximum of a posterior information or can be set manually.</p><h1><a class="nav-anchor" id="Non-Gaussian-random-process-1" href="#Non-Gaussian-random-process-1">Non-Gaussian random process</a></h1><p>If the <span>$f$</span> function errors do not have Gaussian distribution, the strategy <span>$\overrightarrow{\widehat{S}}$</span> can not be calculated analytically in general case.</p><div>\[\overrightarrow{\widehat{S}}[f]=E[\overrightarrow{\varphi}|\overrightarrow{f}]=\int \overrightarrow{\varphi} P(\overrightarrow{\varphi}|\overrightarrow{f}) d\overrightarrow{\varphi}\]</div><p>The posterior probability <span>$P(\overrightarrow{\varphi}|\overrightarrow{f})$</span> should be obtained from MCMC sampling. It is applied in the <code>StatReg.jl</code> using <code>Mamba.jl</code> package.</p><footer><hr/><a class="next" href="getting_started/"><span class="direction">Next</span><span class="title">Getting started</span></a></footer></article></body></html>
